{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc7PbHUJAQZN"
      },
      "source": [
        "# American Trading Automatic Tensorizer (AT-AT)\n",
        "\n",
        "This project is adapting the paper \"MASTER: Market-Guided Stock Transformer for Stock Price Forecasting\" into a suitable model for American stocks and stock formatting.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuBvDHkABMan"
      },
      "source": [
        "# 1. Imports and Environment Setup\n",
        "\n",
        "This section is where the imports for each library are like TensorFlow, PyTorch, and stock data retrieval libraries like “yfinance” for Fortune 200 data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Lgds0l3hAPaj"
      },
      "outputs": [],
      "source": [
        "#################### IMPORTS ####################\n",
        "\n",
        "# Imports for MASTER model reimplementation\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.nn.modules.dropout import Dropout\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "import math\n",
        "\n",
        "# Imports for Base Model files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Sampler\n",
        "import torch.optim as optim\n",
        "\n",
        "# Imports for Fortune 200 Datasets\n",
        "import yfinance as yf\n",
        "\n",
        "# Imports for additional libraries for reimplementation\n",
        "import matplotlib as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQywaTumBbbD"
      },
      "source": [
        "# 2.  Dataset Initialization\n",
        "\n",
        "This is where the data imports, processing, and repackaging for use in AI training and testing is done.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZDSUBZjHdWP",
        "outputId": "be36eac8-655b-4ba0-eccc-92afa9675db3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  10 of 10 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All data processed and saved to CSV.\n",
            "Data split into train, validation, and test sets and saved to CSVs.\n"
          ]
        }
      ],
      "source": [
        "#################### DATASET CREATION ####################\n",
        "\n",
        "# Shortlist of Fortune 500 company tickers\n",
        "tickers = [\"WMT\", \"AMZN\", \"AAPL\", \"UNH\", \"CVS\", \"XOM\", \"GOOGL\", \"MCK\",\n",
        "           \"COR\", \"COST\"]\n",
        "\n",
        "# Preparing CSV file to be later referenced in main\n",
        "universe = 'Fortune200'\n",
        "save_path = f'/content/data/{universe}'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "file_name = f'{save_path}/fortune200_data.csv'\n",
        "\n",
        "batch_size = 10\n",
        "ticker_batches = [tickers[i:i + batch_size] for i in range(0, len(tickers), batch_size)]\n",
        "start_date = \"2014-01-01\"\n",
        "end_date = \"2024-01-01\"\n",
        "lookback_window = 8  # T=8\n",
        "\n",
        "def process_batch(batch_tickers):\n",
        "    # Download data for the current batch of tickers only\n",
        "    data = yf.download(batch_tickers, start=start_date, end=end_date)\n",
        "\n",
        "    # Select relevant columns: Adj Close and Volume\n",
        "    data_adj_close = data['Adj Close']\n",
        "    data_volume = data['Volume']\n",
        "    data_features = pd.concat([data_adj_close, data_volume], axis=1, keys=['Adj Close', 'Volume'])\n",
        "    data_features = data_features.ffill().bfill().dropna(how=\"all\")\n",
        "\n",
        "    # Stack the data, then filter for 'Adj Close' and 'Volume'\n",
        "    adj_close_data = data_features['Adj Close'].stack().reset_index(name='Value')\n",
        "    adj_close_data['Feature'] = 'Adj Close'\n",
        "\n",
        "    volume_data = data_features['Volume'].stack().reset_index(name='Value')\n",
        "    volume_data['Feature'] = 'Volume'\n",
        "\n",
        "    # Concatenate the separate dataframes for Adj Close and Volume\n",
        "    stacked_data = pd.concat([adj_close_data, volume_data], ignore_index=True)\n",
        "\n",
        "    # Reorder columns to ensure consistent order\n",
        "    stacked_data = stacked_data[['Date', 'Ticker', 'Feature', 'Value']]\n",
        "\n",
        "    adj_close_df = stacked_data[stacked_data['Feature'] == 'Adj Close'].pivot(index=['Date', 'Ticker'], columns='Feature', values='Value').reset_index()\n",
        "    volume_df = stacked_data[stacked_data['Feature'] == 'Volume'].pivot(index=['Date', 'Ticker'], columns='Feature', values='Value').reset_index()\n",
        "    data_features = pd.merge(adj_close_df, volume_df, on=['Date', 'Ticker'], how='outer')\n",
        "\n",
        "    # Fill missing data forward and backward to handle missing dates\n",
        "    data_features = data_features.ffill().bfill().fillna(0)\n",
        "    data_features['Date'] = pd.to_datetime(data_features['Date']).dt.tz_localize(None)\n",
        "\n",
        "    # Initial save to CSV file to be added on to\n",
        "    file_name = f'data/{universe}/fortune200_data.csv'\n",
        "    data_features.to_csv(file_name, mode='a', header=not os.path.exists(file_name), index=False)\n",
        "\n",
        "    # Prepare (N, T, F) data structure\n",
        "    max_date = data_features['Date'].max()\n",
        "    date_range = pd.date_range(end=max_date, periods=lookback_window, freq='B')\n",
        "    stock_data = []\n",
        "\n",
        "    # Separate data processing for each ticker\n",
        "    for ticker, group_data in data_features.groupby('Ticker'):\n",
        "        # Reset index to avoid type mismatch issues during reindexing\n",
        "        group_data = group_data.set_index('Date').reindex(date_range, method='ffill').reset_index()\n",
        "        ticker_data = group_data[['Adj Close', 'Volume']].values\n",
        "        stock_data.append(ticker_data)\n",
        "\n",
        "    # Convert stock data list to numpy array\n",
        "    dataset = np.array(stock_data)\n",
        "\n",
        "    # Normalize data per day across stocks\n",
        "    normalized_data = []\n",
        "    for daily_data in dataset:\n",
        "        # Reshape daily_data to (N * T, F) for StandardScaler\n",
        "        reshaped_data = daily_data.reshape(-1, daily_data.shape[-1])\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        # Fit and transform the data, replacing NaNs or infs post-scaling if any remain\n",
        "        normalized_daily_data = scaler.fit_transform(reshaped_data)\n",
        "        normalized_data.append(normalized_daily_data.reshape(daily_data.shape))\n",
        "\n",
        "    return np.array(normalized_data)\n",
        "\n",
        "# Process each batch and save to CSV\n",
        "all_normalized_data = []\n",
        "for batch_num, batch_tickers in enumerate(ticker_batches):\n",
        "    # Process each batch of tickers\n",
        "    batch_normalized_data = process_batch(batch_tickers)\n",
        "    all_normalized_data.append(batch_normalized_data)\n",
        "\n",
        "    # Flatten the 3D array to save as 2D in CSV\n",
        "    flat_data = batch_normalized_data.reshape(-1, batch_normalized_data.shape[-1])\n",
        "\n",
        "# Combine all batch data if needed\n",
        "full_data = np.concatenate(all_normalized_data, axis=0)\n",
        "print(\"All data processed and saved to CSV.\")\n",
        "\n",
        "#################### DATASET SPLITTING ####################\n",
        "\n",
        "# Load the complete dataset\n",
        "dtype_dict = {\n",
        "    \"Date\": \"str\",\n",
        "    \"Ticker\": \"str\"\n",
        "}\n",
        "data_path = f'/content/data/{universe}/fortune200_data.csv'\n",
        "df = pd.read_csv(data_path, dtype=dtype_dict, low_memory=False)\n",
        "\n",
        "# Ensure the output directory exists\n",
        "output_dir = f'/content/data/{universe}'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "# Adjust random_state to ensure reproducibility if needed\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save each split to CSV\n",
        "train_df.to_csv(f'{output_dir}/{universe}_dl_train.csv', index=False)\n",
        "valid_df.to_csv(f'{output_dir}/{universe}_dl_valid.csv', index=False)\n",
        "test_df.to_csv(f'{output_dir}/{universe}_dl_test.csv', index=False)\n",
        "\n",
        "print(\"Data split into train, validation, and test sets and saved to CSVs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icvZj7a4CG8x"
      },
      "source": [
        "# 3. Model Reimplementation\n",
        "\n",
        "For the purposes of project requirements, this assignment has been reimplemented by way of minor algorithmic additions for added functionality. More prominently, most of the code has been changed to achieve the same objective, but in a manner that's different enough to differentiate this model from just copying MASTER itself.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc__MbRIELA7"
      },
      "source": [
        "# 3a. Base Model\n",
        "\n",
        "This section of the model includes the algorithms within base_model.py. These are classes integral to the functionality of the model, including batch samplers, prediction calculators, epoch trainers, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "fwk6fV1-Cmdp"
      },
      "outputs": [],
      "source": [
        "#################### BASE MODEL ####################\n",
        "\n",
        "def calc_ic(pred, label):\n",
        "    dataframe = pd.DataFrame({'pred':pred, 'label':label})\n",
        "    ic = dataframe['pred'].corr(dataframe['label'])\n",
        "    r_ic = dataframe['pred'].corr(dataframe['label'], method='spearman')\n",
        "    return ic, r_ic\n",
        "\n",
        "# HAS BEEN REIMPLEMENTED TO WORK WITH CSV FILES\n",
        "class DailyBatchSamplerRandom(Sampler):\n",
        "    def __init__(self, data_source, batch_size=10, shuffle=False):\n",
        "        self.data_source = data_source\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.num_samples = len(data_source)\n",
        "\n",
        "    def __iter__(self):\n",
        "        indices = np.arange(self.num_samples)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "        # Yield batches based on batch size\n",
        "        for start in range(0, self.num_samples, self.batch_size):\n",
        "            yield indices[start:start + self.batch_size]\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.num_samples + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "class SequenceModel():\n",
        "    def __init__(self, n_epochs, lr, GPU=None, seed=None, train_stop_loss_thred=None, save_path = 'model/', save_prefix= ''):\n",
        "        self.n_epochs = n_epochs\n",
        "        self.lr = lr\n",
        "        self.device = torch.device(f\"cuda:{GPU}\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.seed = seed\n",
        "        self.train_stop_loss_thred = train_stop_loss_thred\n",
        "\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "            torch.manual_seed(self.seed)\n",
        "        self.fitted = False\n",
        "\n",
        "        self.model = None\n",
        "        self.train_optimizer = None\n",
        "\n",
        "        self.save_path = save_path\n",
        "        self.save_prefix = save_prefix\n",
        "\n",
        "\n",
        "    def init_model(self):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"model has not been initialized\")\n",
        "\n",
        "        self.train_optimizer = optim.Adam(self.model.parameters(), self.lr)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def loss_fn(self, pred, label):\n",
        "        mask = ~torch.isnan(label)\n",
        "        loss = (pred[mask]-label[mask])**2\n",
        "        return torch.mean(loss)\n",
        "\n",
        "    # HAS BEEN REIMPLEMENTED TO WORK WITH CSV FILES\n",
        "    def train_epoch(self, data_loader):\n",
        "        self.model.train()\n",
        "        losses = []\n",
        "\n",
        "        for data in data_loader:\n",
        "            data = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
        "\n",
        "            feature = data[:, :, :-1]\n",
        "            label = data[:, -1, -1]\n",
        "\n",
        "            pred = self.model(feature)\n",
        "            loss = self.loss_fn(pred, label)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            self.train_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_value_(self.model.parameters(), 3.0)\n",
        "            self.train_optimizer.step()\n",
        "\n",
        "        return float(np.mean(losses))\n",
        "\n",
        "    # HAS BEEN REIMPLEMENTED TO WORK WITH CSV FILES\n",
        "    def test_epoch(self, data_loader):\n",
        "        self.model.eval()\n",
        "        losses = []\n",
        "\n",
        "        for data in data_loader:\n",
        "            # Convert each batch to a Torch tensor\n",
        "            data = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
        "\n",
        "            # Separate features and labels\n",
        "            feature = data[:, :, :-1]  # All columns except the last one are features\n",
        "            label = data[:, -1, -1]    # Last column is the label\n",
        "\n",
        "            # Forward pass\n",
        "            with torch.no_grad():\n",
        "                pred = self.model(feature)\n",
        "            loss = self.loss_fn(pred, label)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        return float(np.mean(losses))\n",
        "\n",
        "    # HAS BEEN REIMPLEMENTED TO WORK WITH CSV FILES\n",
        "    def _init_data_loader(self, data, shuffle=True, drop_last=True):\n",
        "        sampler = DailyBatchSamplerRandom(data, batch_size=32, shuffle=shuffle)\n",
        "        return DataLoader(data.values, sampler=sampler, drop_last=drop_last)\n",
        "\n",
        "    def load_param(self, param_path):\n",
        "        self.model.load_state_dict(torch.load(param_path, map_location=self.device))\n",
        "        self.fitted = True\n",
        "\n",
        "    # HAS BEEN REIMPLEMENTED TO WORK WITH CSV FILES\n",
        "    def fit(self, dl_train, dl_valid):\n",
        "        # Initiate loaders without the 'Date' column for model training\n",
        "        train_loader = self._init_data_loader(dl_train, shuffle=True, drop_last=True)\n",
        "        valid_loader = self._init_data_loader(dl_valid, shuffle=False, drop_last=True)\n",
        "\n",
        "        # Continue with the rest of the training process\n",
        "        self.fitted = True\n",
        "        best_param = None\n",
        "        for step in range(self.n_epochs):\n",
        "            train_loss = self.train_epoch(train_loader)\n",
        "            val_loss = self.test_epoch(valid_loader)\n",
        "            print(\"Epoch %d, train_loss %.6f, valid_loss %.6f \" % (step, train_loss, val_loss))\n",
        "            best_param = copy.deepcopy(self.model.state_dict())\n",
        "            if train_loss <= self.train_stop_loss_thred:\n",
        "                break\n",
        "\n",
        "        # Ensure the directory exists before saving the file\n",
        "        if not os.path.exists(self.save_path):\n",
        "            os.makedirs(self.save_path)\n",
        "\n",
        "        # Save best model parameters to CSV\n",
        "        df_params = pd.DataFrame.from_dict({k: v.cpu().numpy().flatten() for k, v in best_param.items()}, orient='index')\n",
        "        df_params.to_csv(f'{self.save_path}{self.save_prefix}_master_params.csv')\n",
        "\n",
        "    # HAS BEEN REIMPLEMENTED\n",
        "    def predict(self, dl_test_tensor):\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"model is not fitted yet!\")\n",
        "\n",
        "        test_loader = DataLoader(dl_test_tensor, batch_size=32, shuffle=False, drop_last=False)  # Adjust batch size as needed\n",
        "\n",
        "        preds = []\n",
        "        labels = []\n",
        "\n",
        "        self.model.eval()\n",
        "        for data in test_loader:\n",
        "            data = data.to(self.device)\n",
        "            feature = data[:, :, :-1]\n",
        "            label = data[:, -1, -1]\n",
        "            with torch.no_grad():\n",
        "                pred = self.model(feature).cpu().numpy().ravel()\n",
        "            preds.append(pred)\n",
        "            labels.append(label.cpu().numpy())\n",
        "\n",
        "        predictions = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "\n",
        "        daily_ic, daily_ric = calc_ic(predictions, labels)\n",
        "\n",
        "        metrics = {\n",
        "            'IC': daily_ic,\n",
        "            'ICIR': daily_ic / np.std(predictions) if np.std(predictions) != 0 else 0,\n",
        "            'RIC': daily_ric,\n",
        "            'RICIR': daily_ric / np.std(predictions) if np.std(predictions) != 0 else 0\n",
        "        }\n",
        "\n",
        "        return pd.Series(predictions, index=dl_test_data['Date'][:len(predictions)]), metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yc9noDfS7t3"
      },
      "source": [
        "# 3b. MASTER Model\n",
        "\n",
        "This section is for all of the classes within MASTER - including positional encoding, head attention, parent and classes for MASTER and combining subclasses, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "N7oJ9JaCCnO7"
      },
      "outputs": [],
      "source": [
        "#################### MASTER MODEL ####################\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=3000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.shape[1], :]\n",
        "\n",
        "\n",
        "class SAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.temperature = math.sqrt(self.d_model/nhead)\n",
        "\n",
        "        self.qtrans = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.ktrans = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.vtrans = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        attn_dropout_layer = []\n",
        "        for i in range(nhead):\n",
        "            attn_dropout_layer.append(Dropout(p=dropout))\n",
        "        self.attn_dropout = nn.ModuleList(attn_dropout_layer)\n",
        "\n",
        "        # input LayerNorm\n",
        "        self.norm1 = LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "        # FFN layerNorm\n",
        "        self.norm2 = LayerNorm(d_model, eps=1e-5)\n",
        "        self.ffn = nn.Sequential(\n",
        "            Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            Dropout(p=dropout),\n",
        "            Linear(d_model, d_model),\n",
        "            Dropout(p=dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm1(x)\n",
        "        q = self.qtrans(x).transpose(0,1)\n",
        "        k = self.ktrans(x).transpose(0,1)\n",
        "        v = self.vtrans(x).transpose(0,1)\n",
        "\n",
        "        dim = int(self.d_model/self.nhead)\n",
        "        att_output = []\n",
        "        for i in range(self.nhead):\n",
        "            if i==self.nhead-1:\n",
        "                qh = q[:, :, i * dim:]\n",
        "                kh = k[:, :, i * dim:]\n",
        "                vh = v[:, :, i * dim:]\n",
        "            else:\n",
        "                qh = q[:, :, i * dim:(i + 1) * dim]\n",
        "                kh = k[:, :, i * dim:(i + 1) * dim]\n",
        "                vh = v[:, :, i * dim:(i + 1) * dim]\n",
        "\n",
        "            atten_ave_matrixh = torch.softmax(torch.matmul(qh, kh.transpose(1, 2)) / self.temperature, dim=-1)\n",
        "            if self.attn_dropout:\n",
        "                atten_ave_matrixh = self.attn_dropout[i](atten_ave_matrixh)\n",
        "            att_output.append(torch.matmul(atten_ave_matrixh, vh).transpose(0, 1))\n",
        "        att_output = torch.concat(att_output, dim=-1)\n",
        "\n",
        "        # FFN\n",
        "        xt = x + att_output\n",
        "        xt = self.norm2(xt)\n",
        "        att_output = xt + self.ffn(xt)\n",
        "\n",
        "        return att_output\n",
        "\n",
        "\n",
        "class TAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.qtrans = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.ktrans = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.vtrans = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.attn_dropout = []\n",
        "        if dropout > 0:\n",
        "            for i in range(nhead):\n",
        "                self.attn_dropout.append(Dropout(p=dropout))\n",
        "            self.attn_dropout = nn.ModuleList(self.attn_dropout)\n",
        "\n",
        "        # input LayerNorm\n",
        "        self.norm1 = LayerNorm(d_model, eps=1e-5)\n",
        "        # FFN layerNorm\n",
        "        self.norm2 = LayerNorm(d_model, eps=1e-5)\n",
        "        # FFN\n",
        "        self.ffn = nn.Sequential(\n",
        "            Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            Dropout(p=dropout),\n",
        "            Linear(d_model, d_model),\n",
        "            Dropout(p=dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm1(x)\n",
        "        q = self.qtrans(x)\n",
        "        k = self.ktrans(x)\n",
        "        v = self.vtrans(x)\n",
        "\n",
        "        dim = int(self.d_model / self.nhead)\n",
        "        att_output = []\n",
        "        for i in range(self.nhead):\n",
        "            if i==self.nhead-1:\n",
        "                qh = q[:, :, i * dim:]\n",
        "                kh = k[:, :, i * dim:]\n",
        "                vh = v[:, :, i * dim:]\n",
        "            else:\n",
        "                qh = q[:, :, i * dim:(i + 1) * dim]\n",
        "                kh = k[:, :, i * dim:(i + 1) * dim]\n",
        "                vh = v[:, :, i * dim:(i + 1) * dim]\n",
        "            atten_ave_matrixh = torch.softmax(torch.matmul(qh, kh.transpose(1, 2)), dim=-1)\n",
        "            if self.attn_dropout:\n",
        "                atten_ave_matrixh = self.attn_dropout[i](atten_ave_matrixh)\n",
        "            att_output.append(torch.matmul(atten_ave_matrixh, vh))\n",
        "        att_output = torch.concat(att_output, dim=-1)\n",
        "\n",
        "        # FFN\n",
        "        xt = x + att_output\n",
        "        xt = self.norm2(xt)\n",
        "        att_output = xt + self.ffn(xt)\n",
        "\n",
        "        return att_output\n",
        "\n",
        "# HAS BEEN REIMPLEMENTED\n",
        "class Gate(nn.Module):\n",
        "    def __init__(self, d_input, d_output, beta=1.0):\n",
        "        super(Gate, self).__init__()\n",
        "        self.trans = nn.Linear(d_input, d_output)  # Ensure input and output match\n",
        "        self.d_output = d_output\n",
        "        self.t = beta\n",
        "\n",
        "    def forward(self, gate_input):\n",
        "        # Retain gate_input shape as (batch_size, 1, d_input) before Linear\n",
        "        output = self.trans(gate_input)\n",
        "        output = torch.softmax(output / self.t, dim=-1)\n",
        "        return output  # Returning as-is\n",
        "\n",
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.trans = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, z):\n",
        "        h = self.trans(z) # [N, T, D]\n",
        "        query = h[:, -1, :].unsqueeze(-1)\n",
        "        lam = torch.matmul(h, query).squeeze(-1)  # [N, T, D] --> [N, T]\n",
        "        lam = torch.softmax(lam, dim=1).unsqueeze(1)\n",
        "        output = torch.matmul(lam, z).squeeze(1)  # [N, 1, T], [N, T, D] --> [N, 1, D]\n",
        "        return output\n",
        "\n",
        "# HAS BEEN REIMPLEMENTED\n",
        "class MASTER(nn.Module):\n",
        "    def __init__(self, d_feat=2, d_model=256, t_nhead=4, s_nhead=2,\n",
        "                 T_dropout_rate=0.5, S_dropout_rate=0.5,\n",
        "                 gate_input_start_index=0, gate_input_end_index=None, beta=None):\n",
        "        super(MASTER, self).__init__()\n",
        "\n",
        "        # Ensure they are set as instance variables\n",
        "        self.gate_input_start_index = gate_input_start_index\n",
        "        self.gate_input_end_index = gate_input_end_index\n",
        "\n",
        "        # Set the start and end indices for gating input based on d_feat\n",
        "        self.gate_input_start_index = min(self.gate_input_start_index, d_feat - 1)\n",
        "        self.gate_input_end_index = min(self.gate_input_end_index or d_feat, d_feat)\n",
        "\n",
        "        # Continue with the rest of the initialization as before\n",
        "        self.d_gate_input = max(1, self.gate_input_end_index - self.gate_input_start_index)\n",
        "        self.feature_gate = Gate(self.d_gate_input, 1, beta=beta)\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(2, d_model),\n",
        "            PositionalEncoding(d_model),\n",
        "            TAttention(d_model=d_model, nhead=t_nhead, dropout=T_dropout_rate),\n",
        "            SAttention(d_model=d_model, nhead=s_nhead, dropout=S_dropout_rate),\n",
        "            TemporalAttention(d_model=d_model),\n",
        "            nn.Linear(d_model, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[-1] == 1:\n",
        "            x = x.expand(-1, -1, 2)\n",
        "\n",
        "        src = x[:, :, :self.gate_input_start_index + 1]\n",
        "        gate_input = x[:, :, self.gate_input_start_index:self.gate_input_end_index]\n",
        "\n",
        "        # Apply Gate transformation\n",
        "        gate_output = self.feature_gate(gate_input).expand(-1, src.size(1), -1)\n",
        "        if src.shape == gate_output.shape:\n",
        "            src = src * gate_output\n",
        "\n",
        "        # Reshape `src` to match the Linear layer's expected input shape\n",
        "        src = src.expand(-1, -1, 2)\n",
        "\n",
        "        # Forward pass through layers\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            src = layer(src)\n",
        "\n",
        "        # Removing the previous reshape and gating logic which assumed time dimension\n",
        "        output = self.layers(x)  # input directly\n",
        "        output = output.squeeze(-1)\n",
        "\n",
        "        return output\n",
        "\n",
        "class MASTERModel(SequenceModel):\n",
        "    def __init__(self, d_feat: int = 20, d_model: int = 64, t_nhead: int = 4, s_nhead: int = 2,\n",
        "                 gate_input_start_index=0, gate_input_end_index=None, T_dropout_rate=0.5,\n",
        "                 S_dropout_rate=0.5, beta=5.0, **kwargs):\n",
        "        super(MASTERModel, self).__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.d_feat = d_feat\n",
        "        self.gate_input_start_index = gate_input_start_index\n",
        "        self.gate_input_end_index = gate_input_end_index\n",
        "        self.T_dropout_rate = T_dropout_rate\n",
        "        self.S_dropout_rate = S_dropout_rate\n",
        "        self.t_nhead = t_nhead\n",
        "        self.s_nhead = s_nhead\n",
        "        self.beta = beta\n",
        "        # print(f\"Initializing MASTERModel with gate_input_start_index={self.gate_input_start_index} \"f\"and gate_input_end_index={self.gate_input_end_index}\")\n",
        "        self.init_model()\n",
        "\n",
        "    def init_model(self):\n",
        "        self.model = MASTER(\n",
        "            d_feat=self.d_feat, d_model=self.d_model, t_nhead=self.t_nhead, s_nhead=self.s_nhead,\n",
        "            T_dropout_rate=self.T_dropout_rate, S_dropout_rate=self.S_dropout_rate,\n",
        "            gate_input_start_index=self.gate_input_start_index,\n",
        "            gate_input_end_index=self.gate_input_end_index, beta=self.beta\n",
        "        )\n",
        "        super(MASTERModel, self).init_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1EvALSVUipG"
      },
      "source": [
        "# 3c. main\n",
        "\n",
        "This is where data is split into training and testing sets. The data is then run through the model to make its predictions. This is where MASTER and AT-AT completes its purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jXE9JOhC61t",
        "outputId": "12c4daa9-eeb4-461d-e648-be80edd145fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, train_loss 3191805467000533.500000, valid_loss 3325165369200109.000000 \n",
            "Epoch 1, train_loss 3511633693031731.000000, valid_loss 3324876505295218.500000 \n",
            "Epoch 2, train_loss 4296057642371952.000000, valid_loss 3324053214852749.500000 \n",
            "Epoch 3, train_loss 4150655871411154.500000, valid_loss 3322334435092535.000000 \n",
            "Epoch 4, train_loss 3985706384957897.500000, valid_loss 3319290353694039.000000 \n",
            "Epoch 5, train_loss 4239257348826819.000000, valid_loss 3314431974321398.000000 \n",
            "Epoch 6, train_loss 3633973725029052.000000, valid_loss 3307202402045674.500000 \n",
            "Epoch 7, train_loss 3690435436266677.500000, valid_loss 3297023821341499.000000 \n",
            "Epoch 8, train_loss 3626454526243967.000000, valid_loss 3283487967614004.500000 \n",
            "Epoch 9, train_loss 3978324594420180.500000, valid_loss 3266829890873002.000000 \n",
            "Epoch 10, train_loss 3337798932659207.000000, valid_loss 3248400948563139.000000 \n",
            "Epoch 11, train_loss 3838048805386616.000000, valid_loss 3229380169394161.500000 \n",
            "Epoch 12, train_loss 3612818664102027.500000, valid_loss 3210317094528196.000000 \n",
            "Epoch 13, train_loss 3854504160402406.500000, valid_loss 3191734280940155.500000 \n",
            "Epoch 14, train_loss 3615246545452277.500000, valid_loss 3174591458123698.500000 \n",
            "Epoch 15, train_loss 3404640409130378.000000, valid_loss 3159347103027615.500000 \n",
            "Epoch 16, train_loss 3794569883113352.500000, valid_loss 3143938146737626.000000 \n",
            "Epoch 17, train_loss 3300198192359441.000000, valid_loss 3129801030036494.000000 \n",
            "Epoch 18, train_loss 3548006428896354.000000, valid_loss 3114766355825814.000000 \n",
            "Epoch 19, train_loss 3791836829149050.000000, valid_loss 3101697288121035.500000 \n",
            "Model Trained.\n",
            "Metrics: {'IC': -0.33029510836664694, 'ICIR': -0.03865519422779242, 'RIC': -0.21836947400458667, 'RICIR': -0.025556280481447622}\n"
          ]
        }
      ],
      "source": [
        "#################### TRAINING AND TESTING ####################\n",
        "\n",
        "# Parameters where increase means better performance, but will cost more\n",
        "d_model = 256\n",
        "t_nhead = 4\n",
        "s_nhead = 2\n",
        "\n",
        "d_feat = 2\n",
        "dropout = 0.5  # Can improve generalization if higher but not TOO high\n",
        "beta = 5  # CHANGEABLE PARAMETER\n",
        "n_epoch = 20  # Can improve performance if higher but not TOO high\n",
        "lr = 8e-6  # Can improve performance if higher but not TOO high\n",
        "\n",
        "# NOT CHANEGABLE PARAMETERS\n",
        "gate_input_start_index = 0\n",
        "gate_input_end_index = 2\n",
        "GPU = 0\n",
        "seed = 0\n",
        "train_stop_loss_thred = 0.95\n",
        "\n",
        "# Apply to each dataset\n",
        "dl_train_data = pd.read_csv(f'data/{universe}/{universe}_dl_train.csv').drop(columns=['Date', 'Ticker'])\n",
        "dl_valid_data = pd.read_csv(f'data/{universe}/{universe}_dl_valid.csv').drop(columns=['Date', 'Ticker'])\n",
        "dl_test_data = pd.read_csv(f'data/{universe}/{universe}_dl_test.csv')\n",
        "\n",
        "dl_test_data_numeric = dl_test_data.select_dtypes(include=[float, int])\n",
        "lookback_window = 1\n",
        "num_samples = len(dl_test_data_numeric)\n",
        "dl_test_tensor = torch.tensor(dl_test_data_numeric.values.reshape(num_samples, lookback_window, 2), dtype=torch.float32)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# Set up the model\n",
        "model = MASTERModel(\n",
        "    d_feat=d_feat, d_model=d_model, t_nhead=t_nhead, s_nhead=s_nhead,\n",
        "    T_dropout_rate=dropout, S_dropout_rate=dropout,\n",
        "    beta=beta, gate_input_end_index=gate_input_end_index, gate_input_start_index=gate_input_start_index,\n",
        "    n_epochs=n_epoch, lr=lr, GPU=GPU, seed=seed, train_stop_loss_thred=train_stop_loss_thred,\n",
        "    save_path='model/', save_prefix=universe\n",
        ")\n",
        "\n",
        "model.fit(dl_train_data, dl_valid_data)\n",
        "print(\"Model Trained.\")\n",
        "\n",
        "# Test the model and evaluate metrics\n",
        "predictions, metrics = model.predict(dl_test_tensor)\n",
        "predictions.index = dl_test_data['Date'][:len(predictions)]\n",
        "print(\"Metrics:\", metrics)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}